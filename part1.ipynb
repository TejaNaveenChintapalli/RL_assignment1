{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzueJLSUxIgKs9+naraOAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejaNaveenChintapalli/RL_assignment1/blob/main/part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RZBJfp7K-bT"
      },
      "outputs": [],
      "source": [
        "from matplotlib.text import Annotation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import time\n",
        "import seaborn as sns\n",
        "from google.colab import widgets\n",
        "from gym import spaces\n",
        "import random\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/RL/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LawnMowerDeterministicEnvironment(gym.Env):\n",
        "  metadata = {'render.modes' : []} \n",
        "  def __init__(self, n_rows = 4, n_columns = 4, A_pos = [0, 0], G_pos = [3, 3], max_steps = 15):\n",
        "    self.observation_space = spaces.Discrete(n_rows * n_columns)\n",
        "    self.n_states = n_rows * n_columns\n",
        "    self.n_rows = n_rows\n",
        "    self.n_columns = n_columns\n",
        "    \n",
        "    # Possible actions [Up, Right, Down, Left]\n",
        "    self.action_space = spaces.Discrete(4)\n",
        "    self.n_actions = 4\n",
        "\n",
        "    # Initialization of the State\n",
        "    self.state = np.zeros((n_rows, n_columns))\n",
        "\n",
        "    # Initial position of the Agent\n",
        "    self.agent_pos = [element for element in A_pos]\n",
        "    self.start_pos = [element for element in A_pos]\n",
        "\n",
        "    # Goal position of the Agent\n",
        "    self.goal_pos = [element for element in G_pos]\n",
        "\n",
        "    # Positive Rewards Placement\n",
        "    self.positive_Reward_Pos = [[3, 2], [2, 1], [3, 3]]\n",
        "    self.state[tuple(self.positive_Reward_Pos[0])] = 10\n",
        "    self.state[tuple(self.positive_Reward_Pos[1])] = 14\n",
        "    self.state[tuple(self.goal_pos)] = 20\n",
        "\n",
        "    # Negative Rewards Placement\n",
        "    self.negative_Reward_Pos = [[0, 2], [2, 3]]\n",
        "    self.state[tuple(self.negative_Reward_Pos[0])] = -15\n",
        "    self.state[tuple(self.negative_Reward_Pos[1])] = -13\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.done = False\n",
        "    self.total_reward = 0\n",
        "    self.timestep = 0\n",
        "    self.max_Timestep = max_steps\n",
        "    \n",
        "    \n",
        "    \n",
        "  def reset(self):\n",
        "    # Initialization of the state\n",
        "    self.state = np.zeros((self.n_rows, self.n_columns))\n",
        "\n",
        "    # Initial position of agent\n",
        "    self.agent_pos = [element for element in self.start_pos]\n",
        "\n",
        "    # Positive Rewards Placement\n",
        "    self.state[tuple(self.positive_Reward_Pos[0])] = 5\n",
        "    self.state[tuple(self.positive_Reward_Pos[1])] = 6\n",
        "    self.state[tuple(self.goal_pos)] = 10\n",
        "\n",
        "    # Negative Rewards Placement\n",
        "    self.state[tuple(self.negative_Reward_Pos[0])] = -5\n",
        "    self.state[tuple(self.negative_Reward_Pos[1])] = -6\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.done = False\n",
        "    self.total_reward = 0\n",
        "    self.timestep = 0\n",
        "    \n",
        "  def clip(self):\n",
        "    '''clips the position if it goes out of bounds'''\n",
        "    if self.agent_pos[0] < 0:\n",
        "      self.agent_pos[0] = 0\n",
        "    elif self.agent_pos[0] >= self.n_rows:\n",
        "      self.agent_pos[0] = self.n_rows - 1\n",
        "    if self.agent_pos[1] < 0:\n",
        "      self.agent_pos[1] = 0\n",
        "    elif self.agent_pos[1] >= self.n_columns:\n",
        "      self.agent_pos[1] = self.n_columns - 1\n",
        "    pass\n",
        "\n",
        "  def step(self, action):\n",
        "    '''\n",
        "      Input - Action: 0[Up], 1[Right], 2[Down], 3[Left]\n",
        "      Returns -\n",
        "        agent_pos - Agent current position after applying the move\n",
        "        reward - Reward gained by current action\n",
        "        done - Goal reached status (True/False)\n",
        "        info - Additional info if any\n",
        "    '''\n",
        "    self.state[tuple(self.agent_pos)] = 0\n",
        "\n",
        "    if action == 0: #Up\n",
        "      self.agent_pos[0] -= 1\n",
        "    elif action == 1: #Right\n",
        "      self.agent_pos[1] += 1\n",
        "    elif action == 2: #Down\n",
        "      self.agent_pos[0] += 1\n",
        "    elif action == 3: #Left\n",
        "      self.agent_pos[1] -= 1\n",
        "    \n",
        "    self.clip()\n",
        "\n",
        "    cur_reward = self.state[tuple(self.agent_pos)]\n",
        "    \n",
        "    self.total_reward += cur_reward\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.timestep += 1\n",
        "\n",
        "    self.done = True if (self.agent_pos == self.goal_pos or self.timestep >= self.max_Timestep) else False\n",
        "    info = {}\n",
        "    observation = self.state.flatten()\n",
        "    return observation, cur_reward, self.done, info\n",
        "    \n",
        "\n",
        "\n",
        "  def render(self):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.set_xlim(0, 4)\n",
        "    ax.set_ylim(0, 4)\n",
        "    def plot_image(plot_pos):\n",
        "      #print(plot_pos)\n",
        "      plot_agent, plot_gold, plot_pit, plot_dest = False, False, False, False\n",
        "      if np.array_equal(self.agent_pos, plot_pos):\n",
        "        plot_agent = True\n",
        "      elif np.array_equal(plot_pos, self.goal_pos):\n",
        "        plot_dest = True\n",
        "      elif self.state[tuple(plot_pos)] > 0:  # Gold isn't plotted if it has already been picked by one of the agents.\n",
        "        plot_gold = True\n",
        "      elif self.state[tuple(plot_pos)] < 0:\n",
        "        plot_pit = True\n",
        "      #if any(np.array_equal(self.negative_Reward_Pos[i], plot_pos) for i in range(len(self.negative_Reward_Pos))):\n",
        "       # plot_pit = True\n",
        "      #if any(np.array_equal(self.postive_Reward_Pos[i], plot_pos) for i in range(len(self.postive_Reward_Pos))):\n",
        "       # plot_gold = True\n",
        "      if plot_agent and all(not item for item in\n",
        "                        [plot_gold, plot_pit, plot_dest]):\n",
        "        agent = AnnotationBbox(OffsetImage(plt.imread('wumpus.png'), zoom=0.15),\n",
        "                                       np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "        ax.add_artist(agent)\n",
        "      elif plot_gold and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_pit, plot_dest]):\n",
        "                gold = AnnotationBbox(OffsetImage(plt.imread('gold.png'), zoom=0.15),\n",
        "                                      np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(gold)\n",
        "      elif plot_pit and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_gold, plot_agent, plot_dest]):\n",
        "                pit = AnnotationBbox(OffsetImage(plt.imread('pit.png'), zoom=0.15),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(pit)\n",
        "      elif plot_dest and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_gold, plot_pit, plot_agent]):\n",
        "                pit = AnnotationBbox(OffsetImage(plt.imread('destination.png'), zoom=0.3),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(pit)\n",
        "      else:\n",
        "        pit = AnnotationBbox(OffsetImage(plt.imread('empty.png'), zoom=0.25),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "        ax.add_artist(pit)\n",
        "    coordinates_state_mapping_2 = {}\n",
        "    for j in range(4 * 4):\n",
        "        coordinates_state_mapping_2[j] = np.asarray(\n",
        "            [(j % 4), (int(np.floor(j / 4)))])\n",
        "\n",
        "    # Rendering the images for all states.\n",
        "    for position in coordinates_state_mapping_2:\n",
        "      plot_image(coordinates_state_mapping_2[position])\n",
        "\n",
        "    plt.xticks([0, 1, 2, 3, 4])\n",
        "    plt.yticks([0, 1, 2, 3, 4])\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7xxNHp0oLFd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomAgent(env):\n",
        "  map_action = {0: 'Up', 1: 'Right', 2: 'Down', 3: 'Left'}\n",
        "  for i in range(2):\n",
        "    print(f\"************* Episode {i+1} *************\")\n",
        "    env.reset()\n",
        "    print(\"--------------- initial state ---------------\")\n",
        "    print(\"current state:\", env.agent_pos)\n",
        "    env.render()\n",
        "\n",
        "    for step in range(0,env.max_Timestep):\n",
        "      action = env.action_space.sample()\n",
        "      observation, reward, done, _ = env.step(action)\n",
        "      print(f\"\\n--------------- After Timestep {step+1} ---------------\")\n",
        "      print(\"current state:\", env.agent_pos)\n",
        "      print(\"chosen action:\",  map_action[action])\n",
        "      print(\"reward after current step:\",  reward)\n",
        "      print(\"Total reward:\", env.total_reward)\n",
        "      env.render()\n",
        "      if done:\n",
        "        #env.render()\n",
        "        print(f\"\\n ************* Episode {i+1} ends, Cumulative Rewards = {env.total_reward} ************* \\n\")\n",
        "        print(\"\\n ************* Resetting the environment ************* \\n\")\n",
        "        break"
      ],
      "metadata": {
        "id": "E7m8c5s5Ln9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = LawnMowerDeterministicEnvironment(max_steps=12)\n",
        "RandomAgent(env)"
      ],
      "metadata": {
        "id": "bTvSSJqOL8es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P2MaElbkMW1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}