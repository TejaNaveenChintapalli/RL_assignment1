{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnaTT9/SFVDbMeG0V250F4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejaNaveenChintapalli/RL_assignment1/blob/main/SARSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7zq6-g9n1Jy"
      },
      "outputs": [],
      "source": [
        "class SARSA_():\n",
        "  def __init__(self, Environment, Learning_Rate = 0.01, n_episodes = 500, Epsilon = 1.0, Discount_Factor = 0.3, Epsilon_Decay = 0.01):\n",
        "    self.alpha = Learning_Rate\n",
        "    self.n_episodes = n_episodes\n",
        "    self.epsilon = Epsilon\n",
        "    self.gamma = Discount_Factor\n",
        "    self.epsilon_decay = Epsilon_Decay\n",
        "    self.env = Environment\n",
        "    self.Q_Table = np.random.random((self.env.n_states, self.env.n_actions))\n",
        "    self.Q_Table[self.env.n_states - 1,:] = 0\n",
        "    self.state_map = []\n",
        "    states_count = 0\n",
        "    for i in range(self.env.n_rows):\n",
        "      self.state_map.append([])\n",
        "      for j in range(self.env.n_columns):\n",
        "        self.state_map[i].append(states_count)\n",
        "        states_count += 1\n",
        "    self.games_reward = []\n",
        "    self.decay_growth = []\n",
        "    self.evaluation_results = []\n",
        "    pass\n",
        "  def plot_graph(self, data=None, x_label=\"\", y_label=\"\", title=\"\"):\n",
        "    '''Plotting graph with given data'''\n",
        "    if data is None:\n",
        "      print(\"There isn't any data to plot\")\n",
        "      return \n",
        "    print(\"\\n\")\n",
        "    plt.plot(data)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.show()\n",
        "    pass\n",
        "  def greedy(self, cur_state):\n",
        "    ''' Greedy Policy'''\n",
        "    return np.argmax(self.Q_Table[cur_state])\n",
        "  \n",
        "  def epsilon_greedy(self, cur_state):\n",
        "    '''Epsilon greedy policy'''\n",
        "    if np.random.uniform(0,1) < self.epsilon:\n",
        "        return self.env.action_space.sample()\n",
        "    else:\n",
        "        return self.greedy(cur_state)\n",
        "  \n",
        "  def train_Model(self):\n",
        "    for episode in range(self.n_episodes):\n",
        "      self.env.reset()\n",
        "      state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "      done = False\n",
        "      self.epsilon = max(0.01, self.epsilon - (self.epsilon * self.epsilon_decay))\n",
        "      \n",
        "      action = self.epsilon_greedy(state) \n",
        "      # Running actions until environment reaches final goal or exceeds max timesteps\n",
        "      while not done:\n",
        "        observation, cur_reward, done, _ = self.env.step(action)\n",
        "        next_state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "        next_action = self.epsilon_greedy(next_state)\n",
        "        prev_q = tuple([state, action])\n",
        "        cur_q = tuple([next_state, next_action])\n",
        "        \n",
        "        G = cur_reward + self.gamma * self.Q_Table[cur_q]\n",
        "        self.Q_Table[prev_q] = self.Q_Table[prev_q] + self.alpha * (G - self.Q_Table[prev_q])\n",
        "\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "        if done:\n",
        "          self.games_reward.append(self.env.total_reward)\n",
        "          self.decay_growth.append(self.epsilon)\n",
        "  def evaluate_model(self, test_n_episodes = 10):\n",
        "    output_grid = widgets.Grid(1,1)\n",
        "    for ep in range(test_n_episodes):\n",
        "      self.env.reset()\n",
        "      state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "      done = False\n",
        "      tot_rew = 0\n",
        "      with output_grid.output_to(0,0):\n",
        "        self.env.render()\n",
        "        time.sleep(0.2)\n",
        "        output_grid.clear_cell()\n",
        "      while not done:\n",
        "        action = np.argmax(self.Q_Table[state])\n",
        "        observation, rew, done, _ = self.env.step(action) \n",
        "        state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "\n",
        "        tot_rew += rew\n",
        "        with output_grid.output_to(0,0):\n",
        "          self.env.render()\n",
        "          time.sleep(0.2)\n",
        "          output_grid.clear_cell()\n",
        "        if done:\n",
        "          self.evaluation_results.append(tot_rew)\n",
        "    pass\n"
      ]
    }
  ]
}