{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTRFs9Ebfv/q8Tf0TW8iD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejaNaveenChintapalli/RL_assignment1/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN1YdQd7pKLK"
      },
      "outputs": [],
      "source": [
        "class Q_Learning():\n",
        "  def __init__(self, environment,learning_rate = 0.01, n_episodes = 1500, epsilon = 1.0, discount_factor = 0.3, epsilon_decay = 0.01):\n",
        "    self.alpha = learning_rate\n",
        "    self.n_episodes = n_episodes\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = discount_factor\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.env = environment\n",
        "    \n",
        "    self.Q_Table = np.random.random((self.env.n_states, self.env.n_actions))\n",
        "    self.Q_Table[self.env.n_states - 1,:] = 0\n",
        "    \n",
        "    self.state_map = []\n",
        "    states_count = 0\n",
        "    for i in range(self.env.n_rows):\n",
        "      self.state_map.append([])\n",
        "      for j in range(self.env.n_columns):\n",
        "        self.state_map[i].append(states_count)\n",
        "        states_count += 1\n",
        "    self.games_reward = []\n",
        "    self.decay_growth = []\n",
        "    self.evaluation_results = []\n",
        "    pass\n",
        "  def plot_graph(self, data=None, x_label=\"\", y_label=\"\", title=\"\"):\n",
        "    if data is None:\n",
        "      print(\"There isn't any data to plot\")\n",
        "      return \n",
        "    print(\"\\n\")\n",
        "    plt.plot(data)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.show()\n",
        "    pass\n",
        "  def greedy(self, cur_state):\n",
        "    return np.argmax(self.Q_Table[cur_state])\n",
        "  \n",
        "  def epsilon_greedy(self, cur_state):\n",
        "    '''Epsilon greedy policy'''\n",
        "\n",
        "    if np.random.uniform(0,1) < self.epsilon:\n",
        "        return self.env.action_space.sample()\n",
        "    else:\n",
        "        return self.greedy(cur_state)\n",
        "  \n",
        "  def train_Model(self):\n",
        "    for episode in range(self.n_episodes):\n",
        "      self.env.reset()\n",
        "      state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "      done = False\n",
        "      tot_rew = 0\n",
        "      \n",
        "      self.epsilon = max(0.01, self.epsilon - (self.epsilon * self.epsilon_decay))\n",
        "      action = self.epsilon_greedy(state) \n",
        "      \n",
        "      while not done:\n",
        "        observation, rew, done, _ = self.env.step(action)\n",
        "        next_state = self.state_map[self.env.agent_pos[0]][self.env.agent_pos[1]]\n",
        "        next_action = self.epsilon_greedy(next_state)\n",
        "\n",
        "        gamma_multiplier = self.Q_Table[next_state][self.greedy(next_action)]\n",
        "        self.Q_Table[state][action] = self.Q_Table[state][action] + self.alpha * (rew +\n",
        "                                                                                     self.gamma * gamma_multiplier -\n",
        "                                                                                     self.Q_Table[state][action])\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "        tot_rew += rew\n",
        "\n",
        "        if done:\n",
        "          self.games_reward.append(tot_rew)\n",
        "          self.decay_growth.append(self.epsilon)\n",
        "  \n",
        "  def evaluate_model(self, test_n_episodes = 10):\n",
        "    output_grid = widgets.Grid(1,1)\n",
        "    for ep in range(test_n_episodes):\n",
        "      env.reset()\n",
        "      state = self.state_map[env.agent_pos[0]][env.agent_pos[1]]\n",
        "      done = False\n",
        "      tot_rew = 0\n",
        "      with output_grid.output_to(0,0):\n",
        "        env.render()\n",
        "        time.sleep(0.2)\n",
        "        output_grid.clear_cell()\n",
        "      while not done:\n",
        "        action = self.greedy(state)\n",
        "        observation, rew, done, _ = env.step(action) \n",
        "        state = self.state_map[env.agent_pos[0]][env.agent_pos[1]]\n",
        "\n",
        "        tot_rew += rew\n",
        "        with output_grid.output_to(0,0):\n",
        "          env.render()\n",
        "          time.sleep(0.2)\n",
        "          output_grid.clear_cell()\n",
        "        if done:\n",
        "          self.evaluation_results.append(tot_rew)\n",
        "    pass\n",
        "    \n"
      ]
    }
  ]
}